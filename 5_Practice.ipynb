{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1018b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f07ee",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'><b>1. </b>이제까지 코드는 numpy 형식으로 되어있었습니다. 이를 Pytorch기반의 코드로 바꿔보세요. torch.nn.Linear()나 nn.ReLU()등의 High-level API를 사용하지 마시고, tensor 연산 기반의 코드로 low-level단에서 작성해보세요.</span>\n",
    "\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 1. Activation function들을 Pytorch코드로 변환하셔야 됩니다. (1번문제의 결과물을 활용하셔도 되고, nn.ReLU()와 같은 pytorch에서 제공하는 함수를 사용하셔도 됩니다.)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 2. network에 있는 numpy array를 Pytorch tensor 형태로 변환하세요.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 3. MNIST mini-batch data는 현재 numpy array인데 Pytorch tensor 형태로 변환하세요.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 4. Pytorch에서 텐서곱은 torch.matmul()입니다.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 5. Pytorch에서 Tensor를 numpy array형태로 변경하는 방법은 .numpy()이다.</span>\n",
    "```python\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.numpy() # numpy array형태로 변경됨\n",
    "```\n",
    "\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 6. np.argmax()와 torch.argmax()는 같은 역할을 한다.</span>\n",
    "\n",
    "```python\n",
    "a = torch.rand(size=(5,3))\n",
    "print(a)\n",
    "# 결과값\n",
    "#tensor([[0.6298, 0.9776, 0.4705],\n",
    "#        [0.4715, 0.6208, 0.1938],\n",
    "#        [0.5101, 0.3516, 0.7683],\n",
    "#        [0.5044, 0.5985, 0.1055],\n",
    "#        [0.9975, 0.6862, 0.2044]])\n",
    "\n",
    "torch.argmax(a, dim=1)\n",
    "# 결과값\n",
    "# tensor([1, 1, 2, 1, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4f214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd103f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_torch(x):\n",
    "    \"\"\"\n",
    "    return sigmoid output\n",
    "    \"\"\"\n",
    "    result = 1/(1+torch.exp(-x))\n",
    "    return result\n",
    "    \n",
    "def relu_torch(x):\n",
    "    \"\"\"\n",
    "    return ReLU output\n",
    "    \"\"\"\n",
    "    result = torch.maximum(x, torch.Tensor([0]))\n",
    "    return result\n",
    "\n",
    "def softmax1_torch(x):\n",
    "    exp_sum = torch.sum(torch.exp(x))\n",
    "    result = torch.exp(x)/exp_sum\n",
    "    return result\n",
    "\n",
    "def softmax2_torch(x):\n",
    "    c=torch.max(x)\n",
    "    exp_x = torch.exp(x-c) # overflow 대책\n",
    "    exp_sum = torch.sum(exp_x)\n",
    "    result = exp_x/exp_sum\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338d1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(device):\n",
    "    fpath = 'sample_weights/sample_weight.pkl'\n",
    "    with open(fpath, 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    # network변수안의 모든 key에 대해서 torch.Tensor로 형태 변경\n",
    "    # GPU사용할때를 위해서 해당 tensor를 device로 옮겨놓기\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eff207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(X, y, start_idx, end_idx):\n",
    "    x_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    # x_batch, y_batch를 torch.Tensor로 형태 변경\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3aea71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(network, x):\n",
    "    w1, w2, w3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # torch의 행렬곱과 activation function을 활용하여\n",
    "    # forward propagation 나타내기\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada21bca",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2242300727.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    x_batch =\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(x_trn, y_trn), (x_tst, y_tst) = load_mnist(flatten=True, normalize=False)\n",
    "network = init_network(device)\n",
    "batch_size = 100\n",
    "\n",
    "results = []\n",
    "for i in range(0,x_trn.shape[0],batch_size):\n",
    "    # batch data를 생성하고 device로 올리기 (GPU? CPU?)\n",
    "    x_batch = \n",
    "    \n",
    "    # mini batch데이터에 대해서 forward propagation한 결과를 얻기 (pred_score_batch)\n",
    "    pred_score_batch = \n",
    "    # torch의 argmax()를 활용해서 모델의 예측하는 레이블을 얻기(pred_label_batch)\n",
    "    pred_label_batch = \n",
    "    # pred_label_batch를 numpy array형태로 변경\n",
    "    pred_label_batch = \n",
    "    results.extend(pred_label_batch)\n",
    "    \n",
    "print(f'미리 학습한 모델의 정확도는 {100*np.sum(results==y_trn) / len(y_trn):.2f}%입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17926871",
   "metadata": {},
   "source": [
    "실습 파일 \"3. FFNN Training (with high-level API)\"의 마지막 셀에서 1-(4), 2-(3)을 보면 (line 21, 58) loss를 구할 때 F.nll_loss()를 사용한 것을 볼 수 있다. 또한, 모델을 정의한 셀에서 마지막이 log_softmax()함수를 사용한 것을 알 수 있다. 그러나, lecture note에는 최종 output layer에서 softmax를 취한 뒤, cross entropy loss를 사용하여 loss를 계산한다. 왜 이론과 실제 코드를 다르게 작성한 것일까? 이는 잘못된 코드가 아니라 pytorch에서 cross entropy loss를 계산하는 방식때문에 이렇게 코드를 작성한 것이다. 다음 사이트를 참고하여 F.nll_loss를 사용할 때와 F.cross_entropy를 사용할 때, output layer에 어떤 activation function을 취해야 하는지 설명하시오. 또한, 실습 파일 \"3. FFNN Training (with high-level API)\"의 내용에서 loss를 F.cross_entropy()로 구하도록 코드를 변경하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec1df41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
