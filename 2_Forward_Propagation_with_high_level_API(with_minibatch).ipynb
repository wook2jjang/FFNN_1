{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef202a3",
   "metadata": {},
   "source": [
    "## 이 실습파일에서는 다음 high-level API를 사용하여 forward Propagation을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde318c1",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>torchvision.datasets: </b> pytorch에서 몇가지 sample data를 제공 (MNIST, Fashion-MNIST, CIFAR10, ...)</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>torchvision.transforms: </b>모델을 데이터에 입력하기 전에 전처리 하는 함수들을 제공</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>torch.utils.data.DataLoader: </b>Mini-batch data를 자동으로 생성하는 역할</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326e5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53076434",
   "metadata": {},
   "source": [
    "### 1. Dataset 및 DataLoader생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a02af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "# (The MNIST dataset is also provided in torchvision)\n",
    "# (Check whether the directory(data_path) exists. If not, create a new directory with that name.)\n",
    "data_path = 'data2'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "# data transform method\n",
    "# Following example: Transform the numpy arry data to torch tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "# create a dataset (by torchvision.datasets)\n",
    "trn_dset = datasets.MNIST(root=data_path, \n",
    "                          train=True, \n",
    "                          transform=transform, \n",
    "                          download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, \n",
    "                          train=False, \n",
    "                          transform=transform, \n",
    "                          download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191bc978",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>DataLoader생성하기(train, test data 모두)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9592f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59680"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "373*160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90c0ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "trn_loader = DataLoader(dataset=trn_dset, # 학습데이터로 mini-batch생성기를 만들거야\n",
    "                        batch_size=BATCH_SIZE, # mini-batch의 크기는 BATCH_SIZE이고\n",
    "                        shuffle=False, # 데이터의 순서를 섞어서 만들어줘\n",
    "                        drop_last=False) # MIni-batch를 만들고 남는 데이터는 버려줘\n",
    "\n",
    "# tst_loader = DataLoader(dataset=tst_dset, # 시험데이터로 mini-batch생성기를 만들거야\n",
    "#                         batch_size=BATCH_SIZE, # mini-batch의 크기는 BATCH_SIZE이고\n",
    "#                         shuffle=False, # 데이터의 순서를 섞지말고 만들어줘\n",
    "#                         drop_last=False) # MIni-batch를 만들고 남는 데이터도 활용해줘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86166724",
   "metadata": {},
   "source": [
    "## 2. Model을 만드는 class 생성하기\n",
    "\n",
    "### (1) 네트워크 구조 짜기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Input(784 dim) -> 1st hidden layer (50 dim) -> 2nd hidden layer (100 dim) -> output(10 dim)</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>bias term 존재</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>hidden layer에는 sigmoid, output layer에는 softmax activation function</span>\n",
    "\n",
    "### (2) Input Flow 짜기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 1.</b> (데이터 수 * 784)크기의 행렬(X)을 네트워크에 입력</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 2.</b> 1st hidden layer를 통과.  h1 = Matmul(X, W1) + b1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>W1: (784*50) 크기의 행렬, b1: (50, )크기의 행렬, h1: (데이터 수 * 50) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 3.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 4.</b> 2nd hidden layer를 통과. h2 = Matmul(h1, W2) + b2</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W2: (50*100) 크기의 행렬, b2 = (100, )크기의 행렬, h2: (데이터 수 * 100) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 5.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 6.</b> output layer를 통과. h3 = Matmul(h2, W3) + b3</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W3: (100*10) 크기의 행렬, b3 = (10, )크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 7.</b> Activation함수 통과 (softmax)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57efe9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module): # nn.Module을 상속받아서 MyNet이라는 class를 생성한다.\n",
    "    def __init__(self, in_dim, h1_dim, h2_dim, out_dim):\n",
    "        super(MyNet, self).__init__() # 요건 습관처럼 쓰세요.\n",
    "        \n",
    "        # (1) 네트워크 구조 짜기 (with 단위 모듈)\n",
    "        self.fc1 = nn.Linear(in_dim, h1_dim) # input->1st hidden layer에 대한 FFNN\n",
    "        self.fc2 = nn.Linear(h1_dim, h2_dim) # 1st->2nd hidden layer에 대한 FFNN\n",
    "        self.fc3 = nn.Linear(h2_dim, out_dim) # 2nd->output layer에 대한 FFNN\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (2) Input Flow 짜기\n",
    "        h1 = self.fc1(x) # step2\n",
    "        h1 = F.sigmoid(h1) # step3\n",
    "        h2 = self.fc2(h1) # step4\n",
    "        h2 = F.sigmoid(h2) # step5\n",
    "        h3 = self.fc3(h2) # step6\n",
    "        out = F.softmax(h3, dim=1) # step7\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c785249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4628484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039626f8",
   "metadata": {},
   "source": [
    "### 3. MyNet 클래스로부터 객체(object) 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f112af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNet(in_dim=784, # 입력하는 데이터의 차원 (784)\n",
    "            h1_dim=50, # 1st hidden layer의 neuron 수 (50)\n",
    "            h2_dim=100, # 2nd hidden layer의 neuron 수 (100)\n",
    "            out_dim=10) # data의 class 수 = output layer의 neuron 수(10)\n",
    "net = net.to(device) # 'gpu' 혹은 'cpu'에 network를 올려놓읍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81248e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNet(\n",
       "  (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba4ba9",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Appendix: </b>기존에 학습한 모델을 net의 parameter들에 덮어씌우기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>(보통 이런식으로 안합니다만, 여기서는 그냥 그러려니 합시다.)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6130dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'sample_weights/sample_weight.pkl'\n",
    "with open(fpath, 'rb') as f:\n",
    "    network = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55e5a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc1.weight = nn.Parameter(torch.Tensor(network['W1'].T).to(device))\n",
    "net.fc2.weight = nn.Parameter(torch.Tensor(network['W2'].T).to(device))\n",
    "net.fc3.weight = nn.Parameter(torch.Tensor(network['W3'].T).to(device))\n",
    "net.fc1.bias = nn.Parameter(torch.Tensor(network['b1']).to(device))\n",
    "net.fc2.bias = nn.Parameter(torch.Tensor(network['b2']).to(device))\n",
    "net.fc3.bias = nn.Parameter(torch.Tensor(network['b3']).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eed80f",
   "metadata": {},
   "source": [
    "### 4. DataLoader로 Minibatch data를 생성하고 예측 결과를 누적하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e65bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 784]) torch.Size([10000])\n",
      "[5 0 4 ... 6 9 7]\n"
     ]
    }
   ],
   "source": [
    "for i, (x_trn, y_trn) in enumerate(trn_loader):\n",
    "    print(x_trn.view(-1, 784).shape, y_trn.shape)\n",
    "    y_pred_score = net(x_trn.view(-1,784))\n",
    "    y_pred_label = torch.argmax(y_pred_score, dim=1)\n",
    "    y_pred_label = y_pred_label.detach().cpu().numpy()\n",
    "    y_real_label = y_trn.detach().cpu().numpy()\n",
    "    print(y_real_label)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdade22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미리 학습한 모델의 정확도는 93.58%입니다.\n"
     ]
    }
   ],
   "source": [
    "trn_pred_results = []\n",
    "trn_real_results = []\n",
    "for i, (x_trn, y_trn) in enumerate(trn_loader):\n",
    "    x_trn = x_trn.view(-1,784).to(device)\n",
    "    y_trn = y_trn.to(device)\n",
    "    y_pred_score = net(x_trn)\n",
    "    y_pred_label = torch.argmax(y_pred_score, dim=1)\n",
    "    y_pred_label = y_pred_label.detach().cpu().numpy()\n",
    "    y_real_label = y_trn.detach().cpu().numpy()\n",
    "    trn_pred_results.extend(y_pred_label)\n",
    "    trn_real_results.extend(y_real_label)\n",
    "    \n",
    "trn_pred_results = np.array(trn_pred_results)\n",
    "trn_real_results = np.array(trn_real_results)\n",
    "print(f'미리 학습한 모델의 정확도는 {100*np.sum(trn_pred_results==trn_real_results) / len(trn_real_results):.2f}%입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f241931",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>이제까지는 결과가 계속 92.52%였는데, 여기서는 93.58%??</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>x data가 조금 다릅니다. 이전까지는 '밑바닥부터 시작하는 딥러닝'에서 제공하는 MNIST data, 여기서는 pytorch에서 제공하는 mnist data</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>원칙적으로는 같아야 하지만, 조금 다른게 있는듯. (중요한 건 아니니까 '그런 상황이 있구나' 하고 생각하시면 됩니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4d576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
