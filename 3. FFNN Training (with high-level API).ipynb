{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87caa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7f34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6a882",
   "metadata": {},
   "source": [
    "## 1. 네트워크 구조 짜기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Input(784 dim) -> 1st hidden layer (50 dim) -> 2nd hidden layer (100 dim) -> output(10 dim)</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>bias term 존재</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>hidden layer에는 sigmoid, output layer에는 softmax activation function</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdee598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_h1, bias=True)\n",
    "        self.fc2 = nn.Linear(dim_h1, dim_h2, bias=True)\n",
    "        self.fc3 = nn.Linear(dim_h2, dim_out, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc278b",
   "metadata": {},
   "source": [
    "## 2. Input Flow 짜기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 1.</b> (데이터 수 * 784)크기의 행렬(X)을 네트워크에 입력</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 2.</b> 1st hidden layer를 통과.  h1 = Matmul(X, W1) + b1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>W1: (784*50) 크기의 행렬, b1: (50, )크기의 행렬, h1: (데이터 수 * 50) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 3.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 4.</b> 2nd hidden layer를 통과. h2 = Matmul(h1, W2) + b2</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W2: (50*100) 크기의 행렬, b2 = (100, )크기의 행렬, h2: (데이터 수 * 100) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 5.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 6.</b> output layer를 통과. h3 = Matmul(h2, W3) + b3</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W3: (100*10) 크기의 행렬, b3 = (10, )크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 7.</b> Activation함수 통과 (softmax)</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4942ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_h1, bias=True)\n",
    "        self.fc2 = nn.Linear(dim_h1, dim_h2, bias=True)\n",
    "        self.fc3 = nn.Linear(dim_h2, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x) # Step 1, 2\n",
    "        h1 = torch.sigmoid(h1) # Step 3\n",
    "        h2 = self.fc2(h1) # Step 4\n",
    "        h2 = torch.sigmoid(h2) # Step 5\n",
    "        out = self.fc3(h2) # Step 6\n",
    "        out = F.log_softmax(out) # Step 7\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cafeec",
   "metadata": {},
   "source": [
    "\n",
    "## 3. pytorch 모델을 학습하기 위한 데이터 셋 생성하기 \n",
    "### (By datasets, transforms (in torchvision))\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>1.</b> 데이터를 다운받을 디렉토리 선언 (optional)</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>2.</b> 데이터를 변환할 방법 선언(예시: numpy array 형태의 데이터를 torch Tensor로</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>torch 모델에 입력하는 데이터도 반드시 torch tensor여야 합니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>3.</b> 데이터 셋 생성(다운로드 여부, 학습/평가 데이터 여부, 데이터 변환 방법 등)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46742b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f59593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data2'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, train=True, transform=transform, download=True)\n",
    "tst_dset = datasets.MNIST(root=data_path, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ada111",
   "metadata": {},
   "source": [
    "## 4. Mini-batch 데이터를 자동으로 생성해주는 DataLoader 생성하기\n",
    "### (By DataLoader class (in torch.utils.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbdd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 2**9\n",
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc701f",
   "metadata": {},
   "source": [
    "## 5. 모델 객체를 생성하고 이 모델을 GPU에서 사용할지 GPU에서 사용할지 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6beec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MyNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b29c1b",
   "metadata": {},
   "source": [
    "## 6. Loss function 과 Optimizer 설정하기 (Create an optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f65e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "my_opt = optim.Adam(params = model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614fc03",
   "metadata": {},
   "source": [
    "## 7. 학습을 해봅시다.\n",
    "\n",
    "### Train the model\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>1.</b> Mini-batch train data에 대해서 다음과 같은 과정을 수행합니다. (모든 train data를 다 입력할 때 까지 = 1 epoch)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(1).</b> (n_data,28,28) 형태의 3차원 tensor를 모델에 입력할 수 있도록 (n_data, 784)로 형태 변환합니다. 동시에 모델이 gpu에 있으면 data도 gpu로 올려줍니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(2).</b> 이전 단계에서 계산되어 남아있는 optimizer의 gradient 값들을 전부 0으로 비워줍니다. (안그럼 계속 누적되어서 계산됩니다!)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(3).</b> Forward Propagation: Mini-batch 데이터를 모델에 입력하여 최종 output값을 계산하는 forward propagation을 진행합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(4).</b> Loss Calculation: 실제 y와 모델이 예측한 y사이의 loss를 구합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(5).</b> Gradient Calculation(with Backprop): Loss로부터 모델의 모든 parameter의 gradient를 계산합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(6).</b> Gradient Descent: 계산된 gradient를 활용하여 각 parameter값을 update합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1-(7).</b> trn_loss에 minibatch loss를 누적해서 계산하기</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d0f4f",
   "metadata": {},
   "source": [
    "### Evaluate the model (for validation)\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>2.</b> Mini-batch validation data에 대해서 다음과 같은 과정을 수행합니다. (단, validation에서는 학습을 진행하면 안되기 때문에, torch.no_grad()가 반드시 필요)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2-(1).</b> (n_data,28,28) 형태의 3차원 tensor를 모델에 입력할 수 있도록 (n_data, 784)로 형태 변환합니다. 동시에 모델이 gpu에 있으면 data도 gpu로 올려줍니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2-(2).</b> Forward Propagation: Mini-batch validation 데이터를 모델에 입력하여 최종 output값을 계산하는 forward propagation을 진행합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2-(3).</b> 매 epoch마다 validation set에서 성능이 어떻게 변하는지 모니터링 하기 위해 모델이 예측한 결과와 loss를 계산합니다. 그리고 그 결과를 계속해서 누적합니다.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3c781",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>3. </b> Mini-batch별 예측 결과와 정답을 누적하여 전체 예측 결과와 전체 정답을 만든 뒤, mean test error와 accuracy를 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6149061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8284\\773842903.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(out) # Step 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 2.277 | Test Loss: 2.223 | Test Acc: 28.120% \n",
      "Epoch: 02 | Time: 0m 11s\n",
      "\tTrain Loss: 2.127 | Test Loss: 1.989 | Test Acc: 49.580% \n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 1.826 | Test Loss: 1.649 | Test Acc: 61.580% \n",
      "Epoch: 04 | Time: 0m 12s\n",
      "\tTrain Loss: 1.505 | Test Loss: 1.359 | Test Acc: 66.780% \n",
      "Epoch: 05 | Time: 0m 12s\n",
      "\tTrain Loss: 1.248 | Test Loss: 1.135 | Test Acc: 74.300% \n",
      "Epoch: 06 | Time: 0m 12s\n",
      "\tTrain Loss: 1.053 | Test Loss: 0.965 | Test Acc: 77.910% \n",
      "Epoch: 07 | Time: 0m 12s\n",
      "\tTrain Loss: 0.903 | Test Loss: 0.831 | Test Acc: 82.160% \n",
      "Epoch: 08 | Time: 0m 12s\n",
      "\tTrain Loss: 0.783 | Test Loss: 0.723 | Test Acc: 84.790% \n",
      "Epoch: 09 | Time: 0m 12s\n",
      "\tTrain Loss: 0.686 | Test Loss: 0.635 | Test Acc: 86.130% \n",
      "Epoch: 10 | Time: 0m 12s\n",
      "\tTrain Loss: 0.607 | Test Loss: 0.566 | Test Acc: 87.420% \n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터를 n_epoch(10)번 반복하여 넣을 때 까지 학습합니다.\n",
    "n_epochs = 10\n",
    "\n",
    "# 매 epoch마다 반복\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    # 매 mini-batch train data마다 반복\n",
    "    for i, (x, y) in enumerate(trn_loader):\n",
    "        # 1-(1): 모델에 입력하기 위해서 데이터의 형태 변환\n",
    "        x = x.view(-1,784).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # 1-(2): 기존에 계산된 gradient를 0으로 reset\n",
    "        my_opt.zero_grad()\n",
    "        \n",
    "        # 1-(3): Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # 1-(4): Loss Calculation\n",
    "        loss = F.nll_loss(y_pred_prob, y, reduction = 'sum')\n",
    "        \n",
    "        # 1-(5): Gradient Calculation(Backprop)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 1-(6): Update parameter\n",
    "        my_opt.step()\n",
    "        \n",
    "        # 1-(7): trn_loss에 mini_batch loss를 누적해서 계산하기\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    trn_loss /= len(trn_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # 매 mini-batch validation data마다 반복\n",
    "        for i, (x, y) in enumerate(tst_loader):\n",
    "            # 2-(1)\n",
    "            x = x.reshape(-1,784).to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # 2-(2)\n",
    "            y_pred_prob = model(x)\n",
    "            y_pred_label = torch.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "            # 2-(3)\n",
    "            loss = F.nll_loss(y_pred_prob, y, reduction='sum')\n",
    "            val_loss += loss\n",
    "            \n",
    "            results_pred.extend(y_pred_label.cpu().detach().numpy())\n",
    "            results_real.extend(y.cpu().detach().numpy())\n",
    "            \n",
    "        # 3.\n",
    "        val_loss /= len(tst_loader.dataset)\n",
    "        results_pred = np.array(results_pred)\n",
    "        results_real = np.array(results_real)\n",
    "        accuracy = np.sum(results_pred == results_real) / len(tst_loader.dataset)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaecbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533740db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3140\\773842903.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.log_softmax(out) # Step 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 2.280 | Test Loss: 2.224 | Test Acc: 32.910% \n",
      "Epoch: 02 | Time: 0m 12s\n",
      "\tTrain Loss: 2.130 | Test Loss: 1.994 | Test Acc: 49.770% \n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 1.833 | Test Loss: 1.654 | Test Acc: 57.720% \n",
      "Epoch: 04 | Time: 0m 12s\n",
      "\tTrain Loss: 1.513 | Test Loss: 1.366 | Test Acc: 66.700% \n",
      "Epoch: 05 | Time: 0m 12s\n",
      "\tTrain Loss: 1.257 | Test Loss: 1.141 | Test Acc: 74.520% \n",
      "Epoch: 06 | Time: 0m 12s\n",
      "\tTrain Loss: 1.061 | Test Loss: 0.969 | Test Acc: 78.420% \n",
      "Epoch: 07 | Time: 0m 12s\n",
      "\tTrain Loss: 0.912 | Test Loss: 0.840 | Test Acc: 81.400% \n",
      "Epoch: 08 | Time: 0m 12s\n",
      "\tTrain Loss: 0.797 | Test Loss: 0.738 | Test Acc: 83.810% \n",
      "Epoch: 09 | Time: 0m 12s\n",
      "\tTrain Loss: 0.705 | Test Loss: 0.655 | Test Acc: 85.540% \n",
      "Epoch: 10 | Time: 0m 12s\n",
      "\tTrain Loss: 0.630 | Test Loss: 0.587 | Test Acc: 86.540% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242852ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
