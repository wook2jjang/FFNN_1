{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1018b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f07ee",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'><b>1. </b>이제까지 코드는 numpy 형식으로 되어있었습니다. 이를 Pytorch기반의 코드로 바꿔보세요. torch.nn.Linear()나 nn.ReLU()등의 High-level API를 사용하지 마시고, tensor 연산 기반의 코드로 low-level단에서 작성해보세요.</span>\n",
    "\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 1. Activation function들을 Pytorch코드로 변환하셔야 됩니다. (1번문제의 결과물을 활용하셔도 되고, nn.ReLU()와 같은 pytorch에서 제공하는 함수를 사용하셔도 됩니다.)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 2. network에 있는 numpy array를 Pytorch tensor 형태로 변환하세요.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 3. MNIST mini-batch data는 현재 numpy array인데 Pytorch tensor 형태로 변환하세요.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 4. Pytorch에서 텐서곱은 torch.matmul()입니다.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 5. Pytorch에서 Tensor를 numpy array형태로 변경하는 방법은 .numpy()이다.</span>\n",
    "```python\n",
    "a = torch.Tensor([1,2,3])\n",
    "a.numpy() # numpy array형태로 변경됨\n",
    "```\n",
    "\n",
    "- <span style = 'font-size:1.1em;line-height:1.3em'>Hint 6. np.argmax()와 torch.argmax()는 같은 역할을 한다.</span>\n",
    "\n",
    "```python\n",
    "a = torch.rand(size=(5,3))\n",
    "print(a)\n",
    "# 결과값\n",
    "#tensor([[0.6298, 0.9776, 0.4705],\n",
    "#        [0.4715, 0.6208, 0.1938],\n",
    "#        [0.5101, 0.3516, 0.7683],\n",
    "#        [0.5044, 0.5985, 0.1055],\n",
    "#        [0.9975, 0.6862, 0.2044]])\n",
    "\n",
    "torch.argmax(a, dim=1)\n",
    "# 결과값\n",
    "# tensor([1, 1, 2, 1, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4f214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d5a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_torch(x):\n",
    "    \"\"\"\n",
    "    return sigmoid output\n",
    "    \"\"\"\n",
    "    result = 1/(1+torch.exp(-x))\n",
    "    return result\n",
    "    \n",
    "def relu_torch(x):\n",
    "    \"\"\"\n",
    "    return ReLU output\n",
    "    \"\"\"\n",
    "    result = torch.maximum(x, torch.Tensor([0]))\n",
    "    return result\n",
    "\n",
    "def softmax1_torch(x):\n",
    "    exp_sum = torch.sum(torch.exp(x))\n",
    "    result = torch.exp(x)/exp_sum\n",
    "    return result\n",
    "\n",
    "def softmax2_torch(x):\n",
    "    c=torch.max(x)\n",
    "    exp_x = torch.exp(x-c) # overflow 대책\n",
    "    exp_sum = torch.sum(exp_x)\n",
    "    result = exp_x/exp_sum\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338d1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(device):\n",
    "    fpath = 'sample_weights/sample_weight.pkl'\n",
    "    with open(fpath, 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    # network변수안의 모든 key에 대해서 torch.Tensor로 형태 변경\n",
    "    # GPU사용할때를 위해서 해당 tensor를 device로 옮겨놓기\n",
    "    for key in network.keys():\n",
    "        network[key] = torch.Tensor(network[key]).to(device)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eff207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(X, y, start_idx, end_idx):\n",
    "    x_batch = X[start_idx:end_idx]\n",
    "    y_batch = y[start_idx:end_idx]\n",
    "    # x_batch, y_batch를 torch.Tensor로 형태 변경\n",
    "    x_batch = torch.Tensor(x_batch)\n",
    "    y_batch = torch.Tensor(y_batch)\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3aea71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(network, x):\n",
    "    w1, w2, w3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    # torch의 행렬곱과 activation function을 활용하여\n",
    "    # forward propagation 나타내기\n",
    "    a1 = torch.matmul(x, w1) + b1\n",
    "    z1 = sigmoid_torch(a1)\n",
    "    a2 = torch.matmul(z1, w2) + b2\n",
    "    z2 = sigmoid_torch(a2)\n",
    "    a3 = torch.matmul(z2, w3) + b3\n",
    "    output = softmax2_torch(a3)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada21bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15572\\936989516.py:5: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:205.)\n",
      "  x_batch = torch.Tensor(x_batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미리 학습한 모델의 정확도는 92.52%입니다.\n"
     ]
    }
   ],
   "source": [
    "(x_trn, y_trn), (x_tst, y_tst) = load_mnist(flatten=True, normalize=False)\n",
    "network = init_network(device)\n",
    "batch_size = 100\n",
    "\n",
    "results = []\n",
    "for i in range(0,x_trn.shape[0],batch_size):\n",
    "    x_batch, _ = get_batch_data(x_trn, y_trn, i, i+batch_size)\n",
    "    # x_batch를 device로 올리기 (GPU? CPU?)\n",
    "    x_batch = x_batch.to(device)\n",
    "    pred_score_batch = forward_propagation(network, x_batch)\n",
    "    # torch의 argmax()를 활용해서 pred_score_batch\n",
    "    pred_label_batch = torch.argmax(pred_score_batch, dim=1)\n",
    "    # pred_label_batch를 numpy array형태로 변경\n",
    "    pred_label_batch = pred_label_batch.cpu().numpy()\n",
    "    results.extend(pred_label_batch)\n",
    "    \n",
    "print(f'미리 학습한 모델의 정확도는 {100*np.sum(results==y_trn) / len(y_trn):.2f}%입니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab4c97",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.2em;line-height:1.5em'><b>2. </b>실습 파일 \"3. FFNN Training (with high-level API)\"의 마지막 셀에서 1-(4), 2-(3)을 보면 (line 21, 58) loss를 구할 때 F.nll_loss()를 사용한 것을 볼 수 있다. 또한, 모델을 정의한 셀에서 마지막이 log_softmax()함수를 사용한 것을 알 수 있다. 그러나, lecture note에는 최종 output layer에서 softmax를 취한 뒤, cross entropy loss를 사용하여 loss를 계산한다. 왜 이론과 실제 코드를 다르게 작성한 것일까? 이는 잘못된 코드가 아니라 pytorch에서 cross entropy loss를 계산하는 방식때문에 이렇게 코드를 작성한 것이다. 다음 사이트를 참고하여 F.nll_loss를 사용할 때와 F.cross_entropy를 사용할 때, output layer에 어떤 activation function을 취해야 하는지 설명하시오. 또한, 실습 파일 \"3. FFNN Training (with high-level API)\"의 내용에서 loss를 F.cross_entropy()로 구하도록 코드를 변경하시오.</span>\n",
    "\n",
    "https://velog.io/@och9854/06-1.-Softmax-Classification\n",
    "\n",
    "https://junstar92.tistory.com/118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e223fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb026d1",
   "metadata": {},
   "source": [
    "## 1. Model Class\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 1.</b> (데이터 수 * 784)크기의 행렬(X)을 네트워크에 입력</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 2.</b> 1st hidden layer를 통과.  h1 = Matmul(X, W1) + b1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>W1: (784*50) 크기의 행렬, b1: (50, )크기의 행렬, h1: (데이터 수 * 50) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 3.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 4.</b> 2nd hidden layer를 통과. h2 = Matmul(h1, W2) + b2</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W2: (50*100) 크기의 행렬, b2 = (100, )크기의 행렬, h2: (데이터 수 * 100) 크기의 행렬</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 5.</b> Activation함수 통과 (sigmoid)</span>\n",
    "    \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Step 6.</b> output layer를 통과. h3 = Matmul(h2, W3) + b3</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>W3: (100*10) 크기의 행렬, b3 = (10, )크기의 행렬</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dd2ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, dim_in=784, dim_h1=50, dim_h2=100, dim_out=10):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, dim_h1, bias=True)\n",
    "        self.fc2 = nn.Linear(dim_h1, dim_h2, bias=True)\n",
    "        self.fc3 = nn.Linear(dim_h2, dim_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.fc1(x) # Step 1, 2\n",
    "        h1 = torch.sigmoid(h1) # Step 3\n",
    "        h2 = self.fc2(h1) # Step 4\n",
    "        h2 = torch.sigmoid(h2) # Step 5\n",
    "        out = self.fc3(h2) # Step 6\n",
    "#         out = F.log_softmax(out) # Step 7\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555ba81",
   "metadata": {},
   "source": [
    "## 2. train() 함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`train()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 모델에 입력하기 적합하도록 x의 형태를 변경하고 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 지난 batch로부터 계산했던 gradient를 초기화(`zero_grad()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산. <b>여기서 loss를 `F.nll_loss()`가 아닌 `F.cross_entropy()`를 사용합니다.</b></span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Backpropagation으로 각 parameter의 gradient를 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> Gradient Descent로 parameter값 update</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> `trn_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 8.</b> 데이터 한 개당 평균 train loss 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8f510e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train() # 모델을 학습모드로!\n",
    "    trn_loss = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "        x = x.view(-1, 784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Step 2. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Step 3. Forward Propagation\n",
    "        y_pred_prob = model(x)\n",
    "        \n",
    "        # Step 4. Loss Calculation\n",
    "        loss = F.cross_entropy(y_pred_prob, y, reduction='sum')\n",
    "        \n",
    "        # Step 5. Gradient Calculation (Backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step 6. Update Parameter (by Gradient Descent)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step 7. trn_loss 변수에 mini-batch loss를 누적해서 합산\n",
    "        trn_loss += loss.item()\n",
    "        \n",
    "    # Step 8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e98eea",
   "metadata": {},
   "source": [
    "## 3. evaluate()함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`evaluate()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 모델에 입력하기 적합하도록 x의 형태를 변경하고 원하는 device에 위치시키기</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산. <b>여기서 loss를 `F.nll_loss()`가 아닌 `F.cross_entropy()`를 사용합니다.</b></span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> 모델이 예측하는 레이블을 산출 (with `torch.argmax()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Minibatch의 실제 레이블(`y`)과 예측 레이블(`y_pred_label`)을 누적하여 저장</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> `eval_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> 데이터 한 개당 평균 evaluation loss와 accuracy 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "925d193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    with torch.no_grad(): # evaluate()함수에는 단순 forward propagation만 할 뿐, gradient 계산 필요 X.\n",
    "        for i, (x, y) in enumerate(data_loader):\n",
    "            # Step 1. mini-batch에서 x,y 데이터를 얻고, 원하는 device에 위치시키기\n",
    "            x = x.view(-1,784).to(device) # x.shape: [batch_size,28,28] -> [batch_size, 784]\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Step 2. Forward Propagation\n",
    "            y_pred_prob = model(x)\n",
    "\n",
    "            # Step 3. Loss Calculation\n",
    "            loss = F.cross_entropy(y_pred_prob, y, reduction='sum')\n",
    "            \n",
    "            # Step 4. Predict label\n",
    "            y_pred_label = torch.argmax(y_pred_prob, dim=1)\n",
    "            \n",
    "            # Step 5. Save real and predicte label\n",
    "            results_pred.extend(y_pred_label.detach().cpu().numpy())\n",
    "            results_real.extend(y.detach().cpu().numpy())\n",
    "            \n",
    "            # Step 6. eval_loss변수에 mini-batch loss를 누적해서 합산\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7a79a",
   "metadata": {},
   "source": [
    "## 4. 매 Epoch에 드는 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf092074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560eea82",
   "metadata": {},
   "source": [
    "## 5. 학습하기\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Dataset과 Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b953e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서도 MNIST데이터를 제공합니다. \n",
    "# 이 데이터를 다운 받을 디렉토리(data_path) 존재 여부를 확인하고 존재하지 않으면 생성 \n",
    "data_path = 'data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "# data 변환 방법 선언 (data transform method)\n",
    "# 아래 예시: numpy형태의 데이터를 받으면 걔를 tensor로 변환해줘!\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# dataset을 생성 (torchvision에서 제공하는 데이터를 다운 받고, 위의 방법대로 변환)\n",
    "trn_dset = datasets.MNIST(root=data_path, \n",
    "                          train=True, \n",
    "                          transform=transform, \n",
    "                          download=True)\n",
    "\n",
    "tst_dset = datasets.MNIST(root=data_path, \n",
    "                          train=False, \n",
    "                          transform=transform, \n",
    "                          download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8d919",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>연산을 수행할 device를 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a53b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347714d",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>모델에 대한 객체 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "777afa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(dim_in=784, dim_h1=50, dim_h2=100, dim_out=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a273dbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>학습한 모델을 저장할 directory 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aac3ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2488cb",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요한 hyperparameter값 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac13e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 2**9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b391e05",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Mini-batch를 자동으로 생성할 DataLoader준비하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77e45c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loader = DataLoader(trn_dset, batch_size = BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "tst_loader = DataLoader(tst_dset, batch_size = BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d6347",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>optimizer 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5067d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaecbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "533740db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 10s\n",
      "\tTrain Loss: 2.275 | Test Loss: 2.224 | Test Acc: 34.550% \n",
      "Epoch: 02 | Time: 0m 11s\n",
      "\tTrain Loss: 2.129 | Test Loss: 1.991 | Test Acc: 48.980% \n",
      "Epoch: 03 | Time: 0m 10s\n",
      "\tTrain Loss: 1.824 | Test Loss: 1.638 | Test Acc: 60.630% \n",
      "Epoch: 04 | Time: 0m 11s\n",
      "\tTrain Loss: 1.487 | Test Loss: 1.335 | Test Acc: 67.650% \n",
      "Epoch: 05 | Time: 0m 10s\n",
      "\tTrain Loss: 1.221 | Test Loss: 1.106 | Test Acc: 73.720% \n",
      "Epoch: 06 | Time: 0m 10s\n",
      "\tTrain Loss: 1.022 | Test Loss: 0.935 | Test Acc: 78.170% \n",
      "Epoch: 07 | Time: 0m 11s\n",
      "\tTrain Loss: 0.875 | Test Loss: 0.806 | Test Acc: 81.650% \n",
      "Epoch: 08 | Time: 0m 10s\n",
      "\tTrain Loss: 0.761 | Test Loss: 0.705 | Test Acc: 84.330% \n",
      "Epoch: 09 | Time: 0m 10s\n",
      "\tTrain Loss: 0.670 | Test Loss: 0.623 | Test Acc: 85.820% \n",
      "Epoch: 10 | Time: 0m 10s\n",
      "\tTrain Loss: 0.597 | Test Loss: 0.559 | Test Acc: 87.200% \n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=tst_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model4.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3efbb78",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Appendix</b> `F.cross_entropy(reduction='sum')`을 하는 이유</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>`train()`, `eval()`함수의 `avg_trn_loss`, `avg_eval_loss` 변수는 각각 데이터 한 개당 평균 loss를 나타내려고 하는 것이다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>이를 구하려면 단일 mini-batch내 각 데이터의 loss를 합산하고, 이를 전체 mini-batch에 대해 전부 계산한뒤, 누적합을 구한다. 그리고 이를 데이터의 개수로 나눠주면, 평균 loss가 나온다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>즉, mini-batch내의 각 데이터의 loss를 합산하여 계산하기 위해 `reduction='sum'`을 사용한 것이다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
